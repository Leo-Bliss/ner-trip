
作者：Miracle
链接：https://www.zhihu.com/question/29641737/answer/243982984
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

+ 1、学习率初始学习率很重要，学习率的变化策略也很重要，
你真正跑过数据就会发现其中的奥秘，适当时候可微调，一般在0.1到0.00001范围，
根据具体情况定。
+ 2、batch size我们用的随机梯度下降是建立在batch基础上的，
合适的batch size对你模型的优化是比较重要的，这个参数倒不需要微调，
在一个大致数量即可，常取2的n次方，太大的batch size会受GPU显存的限制，所以不能无限增大。
+ 3、其他参数比如L1，L2正则化的参数，也就是很多深度学习框架里面的wd参数，
一般默认是0.0001，调整正则化的参数可以根据模型表现来，
过拟合的时候可以适当加大系数，非过拟合的时候可不调这个参数，
毕竟跑一次模型得花不少时间。
epoch，应该要和模型表现搭配，如果你的模型已经过拟合了，你就没必要继续跑了；
相反，如果你的epoch太小，你epoch跑完了，模型的loss还在下降，模型还在优化
，那么你这个epoch就太小了，应该增加。至于卷积核个数，模型层数等个人认为看情况而定，
现在的优秀模型都是很多前沿的研究者经过理论分析和无数次的实验得到的，
一般不会说你调一些卷积核个数或其他一些层结构就会有明显提升，当然有时间你可以试试。